{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J5YiZNCPLVPe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-28 21:12:42--  https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
      "Распознаётся www.dropbox.com (www.dropbox.com)... 162.125.70.18, 198.51.44.6, 198.51.45.6, ...\n",
      "Подключение к www.dropbox.com (www.dropbox.com)|162.125.70.18|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа... 301 Moved Permanently\n",
      "Адрес: /s/raw/fnpq3z4bcnoktiv/positive.csv [переход]\n",
      "--2022-05-28 21:12:43--  https://www.dropbox.com/s/raw/fnpq3z4bcnoktiv/positive.csv\n",
      "Повторное использование соединения с www.dropbox.com:443.\n",
      "HTTP-запрос отправлен. Ожидание ответа... 302 Found\n",
      "Адрес: https://uc068e38b3ad5ab0e3d1965f00e0.dl.dropboxusercontent.com/cd/0/inline/BmKYrrp-rKVGmscbOFVrBMHpsQdkyynGya2drrRRjMgS-ylLXIu_N-F9brSr1u5GoxP09pFbB0BtyvhzZ5i7A-YxTsN1x5-gY5mna9Li0qWGPABKH_0UYnHV0kEXnKBzstLUIJSULCMlCOPeT2R6r8RmVI04DK1BLfygLpOwrlbTCA/file# [переход]\n",
      "--2022-05-28 21:12:43--  https://uc068e38b3ad5ab0e3d1965f00e0.dl.dropboxusercontent.com/cd/0/inline/BmKYrrp-rKVGmscbOFVrBMHpsQdkyynGya2drrRRjMgS-ylLXIu_N-F9brSr1u5GoxP09pFbB0BtyvhzZ5i7A-YxTsN1x5-gY5mna9Li0qWGPABKH_0UYnHV0kEXnKBzstLUIJSULCMlCOPeT2R6r8RmVI04DK1BLfygLpOwrlbTCA/file\n",
      "Распознаётся uc068e38b3ad5ab0e3d1965f00e0.dl.dropboxusercontent.com (uc068e38b3ad5ab0e3d1965f00e0.dl.dropboxusercontent.com)... 198.51.44.6, 198.51.45.6, 198.51.44.70, ...\n",
      "Подключение к uc068e38b3ad5ab0e3d1965f00e0.dl.dropboxusercontent.com (uc068e38b3ad5ab0e3d1965f00e0.dl.dropboxusercontent.com)|198.51.44.6|:443... ошибка: Время ожидания соединения истекло.\n",
      "Подключение к uc068e38b3ad5ab0e3d1965f00e0.dl.dropboxusercontent.com (uc068e38b3ad5ab0e3d1965f00e0.dl.dropboxusercontent.com)|198.51.45.6|:443... ошибка: Время ожидания соединения истекло.\n",
      "Подключение к uc068e38b3ad5ab0e3d1965f00e0.dl.dropboxusercontent.com (uc068e38b3ad5ab0e3d1965f00e0.dl.dropboxusercontent.com)|198.51.44.70|:443... ошибка: Время ожидания соединения истекло.\n",
      "Подключение к uc068e38b3ad5ab0e3d1965f00e0.dl.dropboxusercontent.com (uc068e38b3ad5ab0e3d1965f00e0.dl.dropboxusercontent.com)|198.51.45.70|:443... ошибка: Время ожидания соединения истекло.\n",
      "Подключение к uc068e38b3ad5ab0e3d1965f00e0.dl.dropboxusercontent.com (uc068e38b3ad5ab0e3d1965f00e0.dl.dropboxusercontent.com)|162.125.70.15|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа... 200 OK\n",
      "Длина: 26233379 (25M) [text/plain]\n",
      "Сохранение в каталог: ««positive.csv»».\n",
      "\n",
      "positive.csv        100%[===================>]  25,02M  16,0MB/s    за 1,6s    \n",
      "\n",
      "2022-05-28 21:21:34 (16,0 MB/s) - «positive.csv» сохранён [26233379/26233379]\n",
      "\n",
      "--2022-05-28 21:21:34--  https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
      "Распознаётся www.dropbox.com (www.dropbox.com)... 198.51.44.6, 198.51.45.6, 198.51.44.70, ...\n",
      "Подключение к www.dropbox.com (www.dropbox.com)|198.51.44.6|:443... ошибка: Время ожидания соединения истекло.\n",
      "Подключение к www.dropbox.com (www.dropbox.com)|198.51.45.6|:443... ошибка: Время ожидания соединения истекло.\n",
      "Подключение к www.dropbox.com (www.dropbox.com)|198.51.44.70|:443... ^C\n"
     ]
    }
   ],
   "source": [
    "# если у вас линукс / мак / collab или ещё какая-то среда, в которой работает wget, можно так:\n",
    "#!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
    "#!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DFLtXAZ-LVPq"
   },
   "outputs": [],
   "source": [
    "# считываем данные и заполняем общий датасет\n",
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = ['positive'] * len(positive)\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = ['negative'] * len(negative)\n",
    "df = positive.append(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "j1AEISlBLVP0",
    "outputId": "443eadf2-9df4-4507-f2a5-a64f7968182f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  @first_timee хоть я и школота, но поверь, у на...  positive\n",
       "1  Да, все-таки он немного похож на него. Но мой ...  positive\n",
       "2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...  positive\n",
       "3  RT @digger2912: \"Кто то в углу сидит и погибае...  positive\n",
       "4  @irina_dyshkant Вот что значит страшилка :D\\nН...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111918</th>\n",
       "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111919</th>\n",
       "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111920</th>\n",
       "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111921</th>\n",
       "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111922</th>\n",
       "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     label\n",
       "111918  Но не каждый хочет что то исправлять:( http://...  negative\n",
       "111919  скучаю так :-( только @taaannyaaa вправляет мо...  negative\n",
       "111920          Вот и в школу, в говно это идти уже надо(  negative\n",
       "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...  negative\n",
       "111922  Такси везет меня на работу. Раздумываю приплат...  negative"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZWta7oDgLVP8"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M-AvVt8XLVQD"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zeNA7732LVQJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to /home/ivan/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/genesis.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "import nltk\n",
    "from nltk import collocations \n",
    "nltk.download('genesis')\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OfXiH98XLVRV"
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "noise = stopwords.words('russian') + list(punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lY0cWJ7eLVSJ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIjqSVjpLVSL",
    "outputId": "a75ec748-ab21-4bd0-cd41-5a9fa8362b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['first_timee', 'хоть', 'я', 'и', 'школота', 'но', 'поверь', 'у', 'нас', 'то']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [token for tweet in df.text for token in word_tokenize(tweet) if token not in punctuation]\n",
    "print(len(corpus))\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не', 69472),\n",
       " ('и', 55166),\n",
       " ('в', 52902),\n",
       " ('я', 52818),\n",
       " ('RT', 38070),\n",
       " ('на', 35759),\n",
       " ('http', 32998),\n",
       " ('что', 31541),\n",
       " ('с', 27217),\n",
       " ('а', 26860)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict = Counter(corpus)\n",
    "freq_dict_sorted= sorted(freq_dict.items(), key=lambda x: -x[1])\n",
    "list(freq_dict_sorted)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw0GieJSMU-O"
   },
   "source": [
    "## Задание 1.\n",
    "\n",
    "**Задание**: обучите три классификатора: \n",
    "\n",
    "1) на токенах с высокой частотой \n",
    "\n",
    "2) на токенах со средней частотой \n",
    "\n",
    "3) на токенах с низкой частотой\n",
    "\n",
    "Сравните полученные результаты, оцените какие токены наиболее важные для классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QUQ6kAgPMqNn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 507 350587\n"
     ]
    }
   ],
   "source": [
    "# Подготовим три группы токенов: с высокой, средней и низкой частотой\n",
    "\n",
    "high_tokens = set()\n",
    "med_tokens = set()\n",
    "lit_tokens = set()\n",
    "h = 10000 # если частота выше этого значения будем считать что это токен с высокой частотой\n",
    "m = 500 # если частота ниже этого значения будем считать что это токен с низкой частотой\n",
    "for i in freq_dict_sorted:\n",
    "    if i[1] > h:\n",
    "        high_tokens.add(i[0])\n",
    "    elif i[1] < m:\n",
    "        lit_tokens.add(i[0])\n",
    "    else:\n",
    "        med_tokens.add(i[0])\n",
    "print(len(high_tokens),len(med_tokens),len(lit_tokens))                     \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим фильтр (стопслова) с добавлением токенов средней и низкой частоты, \n",
    "# то есть будем считать только по токенам с высокой частотой\n",
    "sw = noise + list(lit_tokens) + list(med_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.57      0.67     39025\n",
      "    positive       0.42      0.69      0.52     17684\n",
      "\n",
      "    accuracy                           0.61     56709\n",
      "   macro avg       0.61      0.63      0.60     56709\n",
      "weighted avg       0.68      0.61      0.62     56709\n",
      "\n",
      "CPU times: user 1min 7s, sys: 1min 7s, total: 2min 14s\n",
      "Wall time: 47.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=sw)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посчитаем только по токенам с средней частотой\n",
    "sw = noise + list(high_tokens) + list(lit_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.68      0.62     23160\n",
      "    positive       0.75      0.64      0.69     33549\n",
      "\n",
      "    accuracy                           0.66     56709\n",
      "   macro avg       0.66      0.66      0.65     56709\n",
      "weighted avg       0.67      0.66      0.66     56709\n",
      "\n",
      "CPU times: user 1min 9s, sys: 1min 6s, total: 2min 15s\n",
      "Wall time: 46.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=sw)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посчитаем только по токенам с низкой частотой\n",
    "sw = noise + list(high_tokens) + list(med_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.72      0.75     30721\n",
      "    positive       0.70      0.78      0.74     25988\n",
      "\n",
      "    accuracy                           0.74     56709\n",
      "   macro avg       0.75      0.75      0.74     56709\n",
      "weighted avg       0.75      0.74      0.75     56709\n",
      "\n",
      "CPU times: user 1min 17s, sys: 1min 54s, total: 3min 11s\n",
      "Wall time: 35.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=sw)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшая точность получилась на токенах с низкой частотностью"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 2.\n",
    "\n",
    "найти фичи с наибольшей значимостью, и вывести их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00     27831\n",
      "    positive       1.00      1.00      1.00     28878\n",
      "\n",
      "    accuracy                           1.00     56709\n",
      "   macro avg       1.00      1.00      1.00     56709\n",
      "weighted avg       1.00      1.00      1.00     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Оставим знаки пунктуации  \n",
    "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=stopwords.words('russian'))\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# похоже что существенный вклад дают симолы которые участвуют в смайлах. Проверим\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '+',\n",
       " ',',\n",
       " '.',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = list('\"#$%&\\'+,.<=>?@[\\\\]^`{|}~')\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "b_JRuyuRLVSY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00     27798\n",
      "    positive       1.00      1.00      1.00     28911\n",
      "\n",
      "    accuracy                           1.00     56709\n",
      "   macro avg       1.00      1.00      1.00     56709\n",
      "weighted avg       1.00      1.00      1.00     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myfilter = stopwords.words('russian') + p\n",
    "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words= myfilter)\n",
    "bow = vec.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 0.517166587314183), ('\"', 0.5043291188347528), ('#', 0.5032534518330424), ('$', 0.4917737925197059), ('%', 0.49351954716182617), ('&', 0.4909979015676524), (\"'\", 0.49159745366696644), ('(', 0.02629212294344813), (')', 0.9133823555343948), ('*', 0.5107654869597419), ('+', 0.49221463965155443), (',', 0.5025657303073586), ('-', 0.5108712902713854), ('.', 0.5083143769066638), ('/', 0.541924562238798), (':', 0.5491191874305665), (';', 0.4956532472799732), ('<', 0.49159745366696644), ('=', 0.4925320495864854), ('>', 0.49159745366696644), ('?', 0.5023012220282494), ('@', 0.5687633356257384), ('[', 0.49159745366696644), ('\\\\', 0.49159745366696644), (']', 0.49163272143751435), ('^', 0.4973637341515456), ('_', 0.5195295279408912), ('`', 0.49131531150258334), ('{', 0.49161508755224037), ('|', 0.4871184468073851), ('}', 0.49159745366696644), ('~', 0.49161508755224037)]\n"
     ]
    }
   ],
   "source": [
    "pp = []\n",
    "for i in punctuation:\n",
    "    cool_token = i\n",
    "    pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
    "    pp.append((i,accuracy_score(pred, y_test)))\n",
    "print(pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем фичи с наибольшей значимостью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")   -    0.9133823555343948\n",
      "@   -    0.5687633356257384\n",
      ":   -    0.5491191874305665\n",
      "/   -    0.541924562238798\n",
      "_   -    0.5195295279408912\n",
      "!   -    0.517166587314183\n",
      "-   -    0.5108712902713854\n",
      "*   -    0.5107654869597419\n",
      ".   -    0.5083143769066638\n",
      "\"   -    0.5043291188347528\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(pp, key=lambda x: -x[1])[:10]:\n",
    "    print(i[0],'  -   ' ,i[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 3.\n",
    "\n",
    "1) сравнить count/tf-idf/hashing векторайзеры/полносвязанную сетку (построить classification_report)\n",
    "\n",
    "2) подобрать оптимальный размер для hashing векторайзера\n",
    "\n",
    "3) убедиться что для сетки нет переобучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.56      0.71     50114\n",
      "    positive       0.23      1.00      0.37      6595\n",
      "\n",
      "    accuracy                           0.61     56709\n",
      "   macro avg       0.61      0.78      0.54     56709\n",
      "weighted avg       0.91      0.61      0.68     56709\n",
      "\n",
      "CPU times: user 1min 20s, sys: 2min 3s, total: 3min 24s\n",
      "Wall time: 36.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "myfilter = stopwords.words('russian') + p\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words= myfilter)\n",
    "bow = count_vect.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print(classification_report(pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00     27831\n",
      "    positive       1.00      1.00      1.00     28878\n",
      "\n",
      "    accuracy                           1.00     56709\n",
      "   macro avg       1.00      1.00      1.00     56709\n",
      "weighted avg       1.00      1.00      1.00     56709\n",
      "\n",
      "CPU times: user 59.5 s, sys: 1min 13s, total: 2min 12s\n",
      "Wall time: 33.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "myfilter = stopwords.words('russian')\n",
    "tf_vect = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words= myfilter)\n",
    "bow = tf_vect.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(tf_vect.transform(x_test))\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.54      0.52     26117\n",
      "    positive       0.58      0.55      0.56     30592\n",
      "\n",
      "    accuracy                           0.54     56709\n",
      "   macro avg       0.54      0.54      0.54     56709\n",
      "weighted avg       0.55      0.54      0.54     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = HashingVectorizer(n_features=2**4,)\n",
    "bow = vectorizer.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vectorizer.transform(x_test))\n",
    "print(classification_report(pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.61      0.60     27604\n",
      "    positive       0.62      0.62      0.62     29105\n",
      "\n",
      "    accuracy                           0.61     56709\n",
      "   macro avg       0.61      0.61      0.61     56709\n",
      "weighted avg       0.61      0.61      0.61     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = HashingVectorizer(n_features=2**8,)\n",
    "bow = vectorizer.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vectorizer.transform(x_test))\n",
    "print(classification_report(pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.65      0.64     27385\n",
      "    positive       0.67      0.65      0.66     29324\n",
      "\n",
      "    accuracy                           0.65     56709\n",
      "   macro avg       0.65      0.65      0.65     56709\n",
      "weighted avg       0.65      0.65      0.65     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = HashingVectorizer(n_features=2**10,)\n",
    "bow = vectorizer.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vectorizer.transform(x_test))\n",
    "print(classification_report(pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.69      0.68     27317\n",
      "    positive       0.71      0.69      0.70     29392\n",
      "\n",
      "    accuracy                           0.69     56709\n",
      "   macro avg       0.69      0.69      0.69     56709\n",
      "weighted avg       0.69      0.69      0.69     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = HashingVectorizer(n_features=2**12,)\n",
    "bow = vectorizer.fit_transform(x_train)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(vectorizer.transform(x_test))\n",
    "print(classification_report(pred, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gJABxhalLVQu",
    "IaQMCGHFLVQ6",
    "5AJk1B39LVRP",
    "RJlvqWuALVRs",
    "rck5OVqhLVSA",
    "mV3fmzp-LVSU",
    "H5THCOjMLVSg",
    "02s2Vh7MLVSj",
    "b1khxRFDLVSm",
    "sfUmWcAQLVSt",
    "BxvtN-3zLVS5",
    "gyrHhYkgLVTB"
   ],
   "name": "sem1_intro_common.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
